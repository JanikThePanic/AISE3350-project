{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><b>Western University</b></center>\n",
    "## <center><b>Faculty of Engineering</b></center>\n",
    "## <center><b>Department of Electrical and Computer Engineering</b></center>\n",
    "\n",
    "# <center><b>AISE 3350A FW24: Cyber-Physical Systems Theory</b></center>\n",
    "# <center><b>Group 13 - Project</b></center>\n",
    "\n",
    "\n",
    "Students:\n",
    "- Jahangir (Janik) Abdullayev (251283871)\n",
    "- Richard Augustine (251275608)\n",
    "- Matthew Linders (251296414)\n",
    "- Xander Chin  (251314531)\n",
    "- Joseph Kim (251283383)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;As cyber-physical systems become increasingly prevalent in the world, sensors have had to become more complex as well. This has resulted in inspection through the use of computer vision, which is an application of artificial intelligence that is used to interpret visual data like images and videos. Computer vision can be indispensable in many different areas. For instance, in civil engineering computer vision has many uses for structural health monitoring [[1]](#bib), like the process of using sensing technology to evaluate the structural integrity and changing conditions of existing structures over time. Using computer vision, structural health monitoring can be used to detect missing components such as bolts and deterioration that appears visually, with more accuracy and cheaper labour costs than a human.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;However, computer vision is a challenging solution to implement. Success varies greatly based on the quality of the video or image given to the system. Computer vision software may be able to identify an object perfectly in some scenarios, but if the object is rotated or partially occluded, or the colours are darker or desaturated, the software may struggle. In the real world, this makes computer vision quite complicated, as real objects very rarely appear consistent with each other to the extent that a basic computer vision model may expect. Computer vision for counting is a valuable application in the industry as it enables accurate, automated inventory management, reducing the time, cost, and errors associated with manual counting. Its scalability and adaptability make it ideal for diverse use cases, from retail stock tracking to industrial supply chain optimization.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Through this assignment, these challenges are explored more thoroughly in a physical example. This project involved developing a computer vision application to count M&M candies by addressing challenges such as object overlap, inconsistent lighting, and varied appearances. The implementation utilized the FastSAM [[2]](#bib), [[3]](#bib) model, a lightweight and efficient variant of the Segment Anything Model (SAM) [[4]](#bib), chosen for its zero-shot segmentation capabilities. FastSAM allowed the system to accurately segment M&M candies without requiring extensive training data, making it well-suited for real-world variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Assumptions\n",
    "\n",
    "It was assumed that the M&Ms presented would be roughly circular and come in a pre-defined colour which simplifies the classification process. As well, it was assumed that the image quality used for testing would come with sufficient resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "The application was developed from scratch in python 3.12.1 [[5]](#bib) and utilized the FastSAM [[2]](#bib) model for the identification and classification of M&Ms, which was then presented to the user using a GUI made using tkinter [[6]](#bib). The application code can be found in [[7]](#bib).\n",
    "For purposes of running the code, a user would need to install the required dependencies listed in the first code cell, this could be done through pip using:\n",
    "\n",
    "```pip install transformers opencv-python matplotlib```\n",
    "\n",
    "On top of this, a user would need to download the FastSAM model locally from [[8]](#bib) and place the downloaded FastSAM-x.py file alongside this Jupyter notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Breakdown\n",
    "\n",
    "The following code cell imports the necessary dependencies outlined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code dependencies\n",
    "\n",
    "# Fixes version mismatch when using ultralytics.yolo\n",
    "%pip install ultralytics==8.0.100\n",
    "\n",
    "# For CV\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from fastsam import FastSAM, FastSAMPrompt\n",
    "import numpy as np\n",
    "\n",
    "# Also need the FastSAM model which is downloaded from google drive:\n",
    "# https://drive.google.com/file/d/1m1sjY4ihXBU1fZXdQ-Xdj-mDltW-2Rqv/view\n",
    "# Place the downloaded FastSAM-x.py file alongside this jupyter notebook file\n",
    "\n",
    "# For GUI\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tkinter import ttk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper functions are designed to assist in the image processing and colour analysis tasks.\n",
    "\n",
    "The apply_mask function creates a mask from a set of coordinates and uses it to isolate a specific region in an image. The mask is applied to the original image, setting the pixels outside the masked area to zero, thereby enhancing the region of interest for further analysis.\n",
    "\n",
    "The `check_circularity` function calculates the circularity of a contour by assessing its area and perimeter, then fitting an ellipse to the contour to determine its shape. The function combines these metrics to provide a score that reflects how circular the contour is. This is important for identifying round objects like M&Ms in images, as it helps distinguish them from other shapes.\n",
    "\n",
    "The `get_average_color` function computes the average colour of an image by filtering out non-black pixels and calculating their mean RGB values. This average colour is then used for classification, allowing the system to identify and distinguish different coloured M&Ms. If no discernible colour is found, the function returns [0, 0, 0].\n",
    "The `classify_color` function matches an RGB value to a predefined set of colours by calculating the Euclidean distance between the given RGB and reference colours. The colour with the smallest distance is selected, providing a classification of the colour present in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# Applies the mask to passed image\n",
    "def apply_mask(image, xy_array):\n",
    "    # Create empty mask of same size as image\n",
    "    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "    \n",
    "    # Fill polygon defined by xy coordinates with ones\n",
    "    cv2.fillPoly(mask, [xy_array], 1)\n",
    "    \n",
    "    # Apply mask to image\n",
    "    masked_image = image.copy()\n",
    "    masked_image[mask == 0] = 0\n",
    "    \n",
    "    return masked_image\n",
    "\n",
    "# Checks how circular the passed contour is\n",
    "def check_circularity(contour):\n",
    "    # Calculate area and perimeter\n",
    "    area = cv2.contourArea(contour)\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "    \n",
    "    # Circularity using isoperimetric inequality\n",
    "    circularity = 4 * np.pi * area / (perimeter * perimeter)\n",
    "    \n",
    "    # Fit an ellipse and check ratio of axes\n",
    "    if len(contour) >= 5:  # Need at least 5 points to fit ellipse\n",
    "        ellipse = cv2.fitEllipse(contour)\n",
    "        major_axis = max(ellipse[1])\n",
    "        minor_axis = min(ellipse[1])\n",
    "        axis_ratio = minor_axis / major_axis\n",
    "    else:\n",
    "        axis_ratio = 0\n",
    "    \n",
    "    # Combine metrics (weight them equally)\n",
    "    final_score = (circularity + axis_ratio) / 2\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "# Returns average colour of the passed image\n",
    "def get_average_color(img):\n",
    "    pixels = np.array(img)\n",
    "    \n",
    "    # Create mask for non-black pixels (where not all RGB values are 0)\n",
    "    non_black_mask = ~np.all(pixels == 0, axis=2)\n",
    "    \n",
    "    # Only consider non-black pixels for average\n",
    "    valid_pixels = pixels[non_black_mask]\n",
    "    \n",
    "    # Return average of valid pixels, or [0,0,0] if all pixels were black\n",
    "    if len(valid_pixels) > 0:\n",
    "        avg_rgb = np.round(valid_pixels.mean(axis=0)).astype(int)\n",
    "        return avg_rgb\n",
    "    return np.array([0, 0, 0])\n",
    "\n",
    "# Predefined colors\n",
    "def classify_color(rgb):\n",
    "    color_dict = {\n",
    "        'Red': [206, 38, 38],\n",
    "        'Orange': [255, 120, 0],\n",
    "        'Yellow': [255, 255, 0],\n",
    "        'Green': [0, 204, 0],\n",
    "        'Blue': [51, 153, 255],\n",
    "        'Brown': [70, 5, 5],\n",
    "        'White': [255, 255, 255]\n",
    "    }\n",
    "    \n",
    "    distances = {\n",
    "        color: np.sqrt(sum((rgb - np.array(ref_rgb))**2))\n",
    "        for color, ref_rgb in color_dict.items()\n",
    "    }\n",
    "    \n",
    "    return min(distances.items(), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `processImage` function is responsible for processing an image to count different coloured M&Ms using FastSAM. Initially, the function clears the GUI output textbox and notifies the user that image processing has started. If no image path is provided, it alerts the user and terminates the function. If a valid image path is given, the function reads and displays the image using OpenCV (`cv2`) and Matplotlib (`plt`). Next, the FastSAM model is loaded with specified parameters (device, retina masks, image size, confidence, IoU) and applied to the image. The function then initializes a FastSAMPrompt, executes the `everything_prompt()` method for image analysis, and stores the detection results. The main role of this function is to seamlessly integrate image loading, object detection, and visualization, allowing for efficient counting of M&Ms by colour.\n",
    "\n",
    "After processing the image with FastSAM, the `processImage` function continues by evaluating the circularity of each detected mask. Using a predefined circularity threshold (`CIRCULAR_THRESHOLD = 0.75`), it filters out masks that do not resemble M&Ms. For qualifying masks, a binary mask is created from the contour, and this mask is applied to the original image to extract the region of interest. The average colour of this region is computed using the `get_average_color` function, and the colour is classified using the `classify_color` function. Each classified colour is counted and stored in a dictionary that tracks the frequency of each M&M colour detected. The function then updates the GUI output box with these counts, providing a visual summary of the M&M distribution by colour. This process is essential for accurately visualizing and validating object detection in the context of real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main image processing function\n",
    "# Takes the path to the image on the machine along with the reference to the GUI output textbox\n",
    "def processImage(img_url, output_text):\n",
    "\n",
    "    # Handle the case the image path is None\n",
    "    if img_url == None:\n",
    "        # Informs user processing has begun\n",
    "        output_text.delete(\"1.0\", tk.END)  # Clear previous text\n",
    "        output_text.insert(tk.END, \"No image loaded\")\n",
    "        print(\"No image loaded\")\n",
    "        return\n",
    "\n",
    "    # Informs user processing has begun\n",
    "    output_text.delete(\"1.0\", tk.END)  # Clear previous text\n",
    "    output_text.insert(tk.END, \"Processing...\")\n",
    "    \n",
    "    # Load and the image in the terminal\n",
    "    raw_image = cv2.cvtColor(cv2.imread(img_url), cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(raw_image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # Load the fastSAM\n",
    "    modelSAM = FastSAM(\"FastSAM-x.pt\")\n",
    "\n",
    "    # Stores results provided the passed settings\n",
    "    everything_results = modelSAM(\n",
    "        img_url,\n",
    "        device=\"cpu\",\n",
    "        retina_masks=True,\n",
    "        imgsz=384,\n",
    "        conf=0.3,\n",
    "        iou=0.9,\n",
    "    )\n",
    "    prompt_process = FastSAMPrompt(img_url, everything_results, device=\"cpu\")\n",
    "\n",
    "    # Everything prompt\n",
    "    prompt_process.everything_prompt()\n",
    "\n",
    "    num_of_masks = len(everything_results[0])\n",
    "    print(num_of_masks)\n",
    "\n",
    "    # Display images with matplotlib\n",
    "    fig, axes = plt.subplots(nrows=int(np.ceil(num_of_masks / 6)), ncols=6, figsize=(10, 5))\n",
    "\n",
    "    # Flatten the axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    final_dict = {\n",
    "        \"Red\": 0,\n",
    "        \"Orange\": 0,\n",
    "        \"Yellow\": 0,\n",
    "        \"Green\": 0,\n",
    "        \"Blue\": 0,\n",
    "        \"Brown\": 0,\n",
    "        \"White\": 0,\n",
    "    }\n",
    "    for index, r in enumerate(everything_results[0]):\n",
    "        maskCoords = (r.masks.xy)[0]\n",
    "        xy_array = np.array(maskCoords)\n",
    "\n",
    "        CIRCULAR_THRESHOLD = 0.75\n",
    "        \n",
    "        # Checks if the mask is circular enough\n",
    "        if(check_circularity(xy_array) > CIRCULAR_THRESHOLD):   \n",
    "            contour = xy_array.reshape((-1, 1, 2)).astype(np.int32)\n",
    "            # Create binary mask from contour\n",
    "            mask = np.zeros(raw_image.shape[:2], dtype=np.uint8)\n",
    "            cv2.fillPoly(mask, [contour], 255)\n",
    "\n",
    "            # Apply mask to image\n",
    "            masked_image = cv2.bitwise_and(raw_image, raw_image, mask=mask)\n",
    "\n",
    "            # Get bounding box just to determine region of interest\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            result_image = masked_image[y:y+h, x:x+w]\n",
    "\n",
    "            # Get average RGB and classify it as a color\n",
    "            avg_rgb = get_average_color(result_image)\n",
    "            color_category = classify_color(avg_rgb)\n",
    "            final_dict[color_category] += 1\n",
    "\n",
    "            ax = axes[index]\n",
    "            ax.axis(\"off\")\n",
    "            ax.imshow(result_image)\n",
    "\n",
    "    print(final_dict)\n",
    "\n",
    "    # Display results in the GUI output box\n",
    "    output_text.delete(\"1.0\", tk.END)  # Clear previous text\n",
    "    output_text.insert(tk.END, final_dict)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `uploadImage` function allows the user to select an image file through a file dialog, supporting various formats such as PNG, JPG, and BMP. Upon selection, the function resizes the image for display, updates the GUI with the image preview, and provides details about the file path, size, and format in the output textbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle uploading the image\n",
    "def uploadImage():\n",
    "    global file_path\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        filetypes=[(\"Image Files\", \"*.png;*.jpg;*.jpeg;*.bmp;*.gif\")]\n",
    "    )\n",
    "    if file_path:\n",
    "        img = Image.open(file_path)\n",
    "        img.thumbnail((300, 300))  # Resize the image to fit in the window\n",
    "        img_tk = ImageTk.PhotoImage(img)\n",
    "        image_label.config(image=img_tk)\n",
    "        image_label.image = img_tk\n",
    "        file_path_label.config(text=f\"File: {file_path}\")\n",
    "\n",
    "        # Display some information in the text box\n",
    "        output_text.delete(\"1.0\", tk.END)  # Clear previous text\n",
    "        output_text.insert(tk.END, f\"File Path: {file_path}\\n\")\n",
    "        output_text.insert(tk.END, f\"Image Size: {img.size}\\n\")\n",
    "        output_text.insert(tk.END, f\"Image Format: {img.format}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main script initializes a graphical user interface (GUI) for image uploading, processing, and information display. Using the `tkinter` library, it creates a main window with a title, specified dimensions, and various interactive elements. Buttons for uploading an image and processing it trigger the `uploadImage` and `processImage` functions, respectively, providing core functionality for file handling and analysis. The GUI includes an image display area, a label to show the selected file path, and a text box to display metadata or analysis results. By packing these components with appropriate layouts and functionality, the script creates an intuitive interface for interacting with images and viewing their processed outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script for GUI\n",
    "\n",
    "# Initialize the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Image and Info Display GUI\")\n",
    "root.geometry(\"400x600\")\n",
    "\n",
    "file_path = None\n",
    "\n",
    "# Upload image button\n",
    "upload_button = ttk.Button(\n",
    "    root, text=\"Upload Image\", command=uploadImage\n",
    ")\n",
    "upload_button.pack(pady=10)\n",
    "\n",
    "# Upload process button\n",
    "upload_button = ttk.Button(\n",
    "    root, text=\"Process Image\", command=lambda: processImage(file_path, output_text)\n",
    ")\n",
    "upload_button.pack(pady=10)\n",
    "\n",
    "# Image display label\n",
    "image_label = tk.Label(root)\n",
    "image_label.pack(pady=10)\n",
    "\n",
    "# File path label\n",
    "file_path_label = tk.Label(root, text=\"No file selected\", wraplength=300)\n",
    "file_path_label.pack()\n",
    "\n",
    "# Text box for output information\n",
    "output_text = tk.Text(root, height=10, width=40, state=tk.NORMAL)\n",
    "output_text.pack(pady=10)\n",
    "\n",
    "# Run the main loop\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the developed code, a user must first download the dependencies, as explained in the methodology. Once the dependencies are acquired, run the code and the GUI will pop up. When the GUI appears, the user can select the upload image option and select the image file that they wish to select. Once the image has been selected, the user can press the “Process Image” button to execute the code to process the image. The results of the model’s count are printed in the text box as a count of each colour. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"GUI_No_Image.png\" alt=\"GUI with no image\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"GUI_With_Image.png\" alt=\"GUI with image\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During testing, the program also outputs an image with all the masks of the objects it found that resembled M&Ms due to their round shape. To measure the effectiveness of the model, this image of selected masks was compared to the original image. Specifically, the values measured were the true number of candies in the image, the number of objects that the model and classified as M&Ms, the number of M&Ms missed, the colour(s) of the candies missed, the quantity of each colour it says it found, the number of colours misidentified, the total number of objects detected. Using these observations, a variety of metrics were derived. These metrics can be split up into M&M identification metrics and colour identification metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M&M identification metrics: \n",
    "-\tThe number of correct classifications of M&Ms (True positives)\n",
    "-\tThe number of non-M&M objects considered as M&Ms (False positives)\n",
    "-\tThe number of non-M&M objects identified that are discarded (True Negatives)\n",
    "-\tThe number of M&Ms missed (False Negatives)\n",
    "-\tThe percentage of M&Ms identified\n",
    "-\tThe percentage of objects considered as M&M’s that were correct\n",
    "-\tPrecision, Accuracy, Sensitivity, Specificity\n",
    "\n",
    "\n",
    "\n",
    "Colour identification metrics: \n",
    "-\tThe number of incorrect colour classifications\n",
    "-\tThe percentage of correct colour classifications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"results1.png\" alt=\"results1\" width=\"1080\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"results2.png\" alt=\"results2\" width=\"820\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics were gathered from 15 unique testing images including varying numbers of M&Ms, varying backgrounds, the addition of extra objects, and different lighting.\n",
    "\n",
    "In terms of M&M identification, the model does quite well. The model only misses M&Ms in two trials and finds additional non-M&Ms as M&Ms in 5 trials. The missed M&Ms are due to faulty object segmentation where the masks of the M&Ms include too much of the surrounding background to be circular. Most of the extra objects included were perfectly circular, or images of M&Ms on the packaging, which all fit the criteria and given assumption that M&Ms are perfectly circular objects. Only 4 objects thought to be M&Ms were not circular or looked very similar M&Ms. \n",
    "\n",
    "This relationship between True positives and false positives can be summarized by the precision score:\n",
    "\n",
    "$$\n",
    "\\frac{True Positive}{True Positive + False Positive}\n",
    "$$ \n",
    "\n",
    "which this model achieved an average of 96%. Similarly, there is the sensitivity:\n",
    "\n",
    "$$\\frac{True Positive}{True Positive + False Negative}$$ \n",
    "\n",
    "with an average of 99%. Additionally, it is important to note that the model segments all objects and the number of true negatives that are disqualifies is far greater than the number of false negatives. This is symbolized by the specificity:\n",
    "\n",
    "$$\\frac{True Negatives}{True Negatives + False Positives}$$ \n",
    "\n",
    "with an average of 87% and the Negative predictive value:\n",
    "\n",
    "$$\\frac{True Negatives}{True Negatives + False Negatives}$$ \n",
    "\n",
    "with an average of 96%. Finally, all of this is summarized with the accuracy:\n",
    "\n",
    "$$\\frac{True Positives + True Negatives}{True Negatives + True Positives + False Negatives + False Positives}$$ \n",
    "\n",
    "which has an average of 96%. Overall, the model is quite good at distinguishing M&Ms from other objects even with a simple criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, the model does a very poor job determining the colour of the M&M once it has found it. Across the 15 trials, the model correctly identifies the correct colour for an average of 61% of the candies. Typically, the model can accurately identify the red M&Ms, struggles to identify orange, yellow, green, and blue, and disproportionately labels the candies as brown. \n",
    "\n",
    "Three main causes of this discrepancy are a basic understanding of colours, the lighting, and the background of the image. First, the orange M&Ms are often categorized as red, which makes sense as those colours are very similar, especially if the orange colour is in darker lighting. In general, when the lighting is darker, almost all colours become like brown, thus explaining how brown is the only colour sensed when the photo is taken in a dark space. Finally, if the background is light, the model makes more colour errors. This seems to be because the reflection off lighter backgrounds causes the photo to be more saturated, shadowed, and contrasted thus making them look darker and more like brown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Approach Explanation\n",
    "FastSAM was selected for its zero-shot segmentation capabilities [[2]](#bib), which allow it to identify objects in an image without requiring pre-trained labels or additional training data. The segmented regions are further processed using circularity checks [[9]](#bib) and colour classification to identify and count M&Ms based on predefined colour categories. Clearly, this decision led to good results since the model produced an average precision of 96%.\n",
    "### Requirements for Approach\n",
    "To implement this model, some key components were required. A custom colour classifier was implemented to categorize the objects based on their average RGB values, ensuring precise colour identification. Additionally, a GUI was developed using tkinter to provide a user-friendly interface, allowing users to upload images, process them, and view results interactively. These elements worked together to create an automated system for identifying and classifying M&Ms.\n",
    "### Implementation Issues\n",
    "Although the program performs well in most cases, some challenges came up during development. The FastSAM model initially had high sensitivity to overlapping or incomplete masks sometimes resulted in irrelevant or partial regions being segmented. To address this, the circularity threshold was introduced to filter out non-circular shapes. However, this threshold occasionally excluded valid objects, such as slightly deformed or hidden M&Ms. Additionally, while FastSAM's zero-shot segmentation capabilities eliminated the need for training data, its general-purpose design meant that the segmentation output was not always optimized for this specific use case.\n",
    "\n",
    "Another issue was regarding color classification. Predefined RGB values worked effectively for clear and distinct colors, but shades that closely resembled multiple categories (e.g., dark red versus brown) led to occasional misclassifications. Variations in lighting or image quality further complicated color identification. A potential improvement could involve incorporating CMYK color space conversion, which separates color components (cyan, magenta, yellow) from the brightness and shading component (black). This could allow the algorithm to focus on pure color information, reducing the impact of lighting variations. Additionally, refining classification thresholds or training a custom color classification model could further enhance accuracy in challenging conditions.\n",
    "\n",
    "### Potential Societal/Economic Impact\n",
    "This project demonstrates significant societal and economic potential. For example, in the food industry, this system could automate quality control processes, ensuring that items are correctly sorted and packaged by color. This automation would improve efficiency and reduce labor costs, benefiting manufacturers and consumers alike. Additionally, the approach could be applied to other industries, such as recycling or manufacturing, where accurate object identification and sorting are crucial.\n",
    "On the societal side, the automation of such processes raises concerns about job displacement in industries dependent on manual sorting. Furthermore, The FastSAM's ease of use and reliance on general-purpose segmentation reduce the need for extensive training data, lowering entry barriers for deploying similar solutions across various applications. By balancing these benefits and challenges, this project has transformative potential of AI in automation and efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This project helped explore the application of a computer vision model in counting M&M candies, displaying applications of machine learning in the real world. By using the FastSAM model and integrating it to detect for circularity and colour, the model was able to achieve a high level of precision in terms of detecting M&Ms with zero shot segmentation. \n",
    "\n",
    "The results have shown that while the detection of M&Ms was largely successful, with a rare issue being the circularity detection integrated in the FastSAM model producing false negatives. This demonstrated the flawed aspect of circularity detection as it could not detect M&Ms that were occluded or not perfectly circular.\n",
    "\n",
    "Additionally, colour detection was a significant and reoccurring issue with this program. This provided a learning opportunity in the importance of lighting and background colour control for future evaluations. Therefore, future models will aim to disregard the effects of lighting, background and colour similarities through incorporating CMYK colour space conversion.\n",
    "\n",
    "In conclusion, this project helped teach the obstacles faced by the machine learning industry in integrating image recognition technology to real world applications. While this model was not perfect it was largely successful in creating a scalable, efficient and, cost-effective method of object detection that can be used in various industries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"bib\">Bibliography</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]\tZ. Peng, J. Li, H. Hao, and Y. Zhong, “Smart structural health monitoring using computer vision and edge computing,” Engineering Structures, vol. 319, p. 118809, Nov. 2024, doi: [10.1016/j.engstruct.2024.118809](https://doi.org/10.1016/j.engstruct.2024.118809).\n",
    "\n",
    "[2]\tUltralytics, “FastSAM (Fast Segment Anything Model).” Accessed: Dec. 19, 2024. [Online]. Available: https://docs.ultralytics.com/models/fast-sam\n",
    "\n",
    "[3]\tCASIA-IVA-Lab/FastSAM. (Dec. 19, 2024). Python. CASIA-IVA-Lab. Accessed: Dec. 19, 2024. [Online]. Available: https://github.com/CASIA-IVA-Lab/FastSAM\n",
    "\n",
    "[4]\tUltralytics, “SAM (Segment Anything Model).” Accessed: Dec. 19, 2024. [Online]. Available: https://docs.ultralytics.com/models/sam\n",
    "\n",
    "[5]\t“Python Release Python 3.12.1,” Python.org. Accessed: Dec. 19, 2024. [Online]. Available: https://www.python.org/downloads/release/python-3121/\n",
    "\n",
    "[6]\t“tkinter — Python interface to Tcl/Tk,” Python documentation. Accessed: Dec. 19, 2024. [Online]. Available: https://docs.python.org/3/library/tkinter.html\n",
    "\n",
    "[7]\tjanik, JanikThePanic/AISE3350-project. (Dec. 20, 2024). Jupyter Notebook. Accessed: Dec. 19, 2024. [Online]. Available: https://github.com/JanikThePanic/AISE3350-project\n",
    "\n",
    "[8]\t“FastSAM-x.pt - Google Drive.” Accessed: Dec. 19, 2024. [Online]. Available: https://drive.google.com/file/d/1m1sjY4ihXBU1fZXdQ-Xdj-mDltW-2Rqv/view\n",
    "\n",
    "[9]\t“Roundness,” Wikipedia. Oct. 03, 2024. Accessed: Dec. 19, 2024. [Online]. Available: https://en.wikipedia.org/w/index.php?title=Roundness&oldid=1249101195"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
