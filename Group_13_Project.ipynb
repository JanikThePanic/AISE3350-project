{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><b>Western University</b></center>\n",
    "## <center><b>Faculty of Engineering</b></center>\n",
    "## <center><b>Department of Electrical and Computer Engineering</b></center>\n",
    "\n",
    "# <center><b>AISE 3350A FW24: Cyber-Physical Systems Theory</b></center>\n",
    "# <center><b>Group 13 - Project</b></center>\n",
    "\n",
    "\n",
    "Students:\n",
    "- Jahangir (Janik) Abdullayev (251283871)\n",
    "- Richard Augustine (251275608)\n",
    "- Matthew Linders (251296414)\n",
    "- Xander Chin  (251314531)\n",
    "- Joseph Kim (251283383)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;As cyber-physical systems become increasingly prevalent in the world, sensors have had to become more complex as well. This has resulted in inspection through the use of computer vision, which is an application of artificial intelligence that is used to interpret visual data like images and videos. Computer vision can be indispensable in many different areas. For instance, in civil engineering computer vision has many uses for structural health monitoring [[1]](#bib), like the process of using sensing technology to evaluate the structural integrity and changing conditions of existing structures over time. Using computer vision, structural health monitoring can be used to detect missing components such as bolts, and deterioration that appears visually, with more accuracy and cheaper labour costs than a human.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;However, computer vision is a challenging solution to implement. Success varies greatly based on the quality of the video or image given to the system. Computer vision software may be able to identify an object perfectly in some scenarios, but if the object is rotated or partially occluded, or the colours are darker or desaturated, the software may struggle. In the real world, this makes computer vision quite complicated, as real objects very rarely appear consistent with each other to the extent that a basic computer vision model may expect. Computer vision for counting is a valuable application in the industry as it enables accurate, automated inventory management, reducing the time, cost, and errors associated with manual counting. Its scalability and adaptability make it ideal for diverse use cases, from retail stock tracking to industrial supply chain optimization.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Through this assignment, these challenges are explored more thoroughly in a physical example. This project involved developing a computer vision application to count M&M candies by addressing challenges such as object overlap, inconsistent lighting, and varied appearances. The implementation utilized the FastSAM [[2]](#bib), [[3]](#bib) model, a lightweight and efficient variant of the Segment Anything Model (SAM) [[4]](#bib), chosen for its zero-shot segmentation capabilities. FastSAM allowed the system to accurately segment M&M candies without requiring extensive training data, making it well-suited for real-world variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code dependencies\n",
    "\n",
    "# For CV\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from fastsam import FastSAM, FastSAMPrompt\n",
    "import numpy as np\n",
    "\n",
    "# Also need the FastSAM model which is downloaded from google drive:\n",
    "# https://drive.google.com/file/d/1m1sjY4ihXBU1fZXdQ-Xdj-mDltW-2Rqv/view\n",
    "# Place the downloaded FastSAM-x.py file alongside this jupyter notebook file\n",
    "\n",
    "# For GUI\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tkinter import ttk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# Applies the mask to passed image\n",
    "def apply_mask(image, xy_array):\n",
    "    # Create empty mask of same size as image\n",
    "    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "    \n",
    "    # Fill polygon defined by xy coordinates with ones\n",
    "    cv2.fillPoly(mask, [xy_array], 1)\n",
    "    \n",
    "    # Apply mask to image\n",
    "    masked_image = image.copy()\n",
    "    masked_image[mask == 0] = 0\n",
    "    \n",
    "    return masked_image\n",
    "\n",
    "# Checks how circular the passed contour is\n",
    "def check_circularity(contour):\n",
    "    # Calculate area and perimeter\n",
    "    area = cv2.contourArea(contour)\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "    \n",
    "    # Circularity using isoperimetric inequality\n",
    "    circularity = 4 * np.pi * area / (perimeter * perimeter)\n",
    "    \n",
    "    # Fit an ellipse and check ratio of axes\n",
    "    if len(contour) >= 5:  # Need at least 5 points to fit ellipse\n",
    "        ellipse = cv2.fitEllipse(contour)\n",
    "        major_axis = max(ellipse[1])\n",
    "        minor_axis = min(ellipse[1])\n",
    "        axis_ratio = minor_axis / major_axis\n",
    "    else:\n",
    "        axis_ratio = 0\n",
    "    \n",
    "    # Combine metrics (weight them equally)\n",
    "    final_score = (circularity + axis_ratio) / 2\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "# Returns average colour of the passed image\n",
    "def get_average_color(img):\n",
    "    pixels = np.array(img)\n",
    "    \n",
    "    # Create mask for non-black pixels (where not all RGB values are 0)\n",
    "    non_black_mask = ~np.all(pixels == 0, axis=2)\n",
    "    \n",
    "    # Only consider non-black pixels for average\n",
    "    valid_pixels = pixels[non_black_mask]\n",
    "    \n",
    "    # Return average of valid pixels, or [0,0,0] if all pixels were black\n",
    "    if len(valid_pixels) > 0:\n",
    "        avg_rgb = np.round(valid_pixels.mean(axis=0)).astype(int)\n",
    "        return avg_rgb\n",
    "    return np.array([0, 0, 0])\n",
    "\n",
    "# Predefined colors\n",
    "def classify_color(rgb):\n",
    "    color_dict = {\n",
    "        'Red': [206, 38, 38],\n",
    "        'Orange': [255, 120, 0],\n",
    "        'Yellow': [255, 255, 0],\n",
    "        'Green': [0, 204, 0],\n",
    "        'Blue': [51, 153, 255],\n",
    "        'Brown': [70, 5, 5],\n",
    "        'White': [255, 255, 255]\n",
    "    }\n",
    "    \n",
    "    distances = {\n",
    "        color: np.sqrt(sum((rgb - np.array(ref_rgb))**2))\n",
    "        for color, ref_rgb in color_dict.items()\n",
    "    }\n",
    "    \n",
    "    return min(distances.items(), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main image processing function\n",
    "# Takes the path to the image on the machine along with the reference to the GUI output textbox\n",
    "def processImage(img_url, output_text):\n",
    "\n",
    "    # Handle the case the image path is None\n",
    "    if img_url == None:\n",
    "        # Informs user processing has begun\n",
    "        output_text.delete(\"1.0\", tk.END)  # Clear previous text\n",
    "        output_text.insert(tk.END, \"No image loaded\")\n",
    "        print(\"No image loaded\")\n",
    "        return\n",
    "\n",
    "    # Informs user processing has begun\n",
    "    output_text.delete(\"1.0\", tk.END)  # Clear previous text\n",
    "    output_text.insert(tk.END, \"Processing...\")\n",
    "    \n",
    "    # Load and the image in the terminal\n",
    "    raw_image = cv2.cvtColor(cv2.imread(img_url), cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(raw_image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # Load the fastSAM\n",
    "    modelSAM = FastSAM(\"FastSAM-x.pt\")\n",
    "\n",
    "    # Stores results provided the passed settings\n",
    "    everything_results = modelSAM(\n",
    "        img_url,\n",
    "        device=\"cpu\",\n",
    "        retina_masks=True,\n",
    "        imgsz=384,\n",
    "        conf=0.3,\n",
    "        iou=0.9,\n",
    "    )\n",
    "    prompt_process = FastSAMPrompt(img_url, everything_results, device=\"cpu\")\n",
    "\n",
    "    # Everything prompt\n",
    "    prompt_process.everything_prompt()\n",
    "\n",
    "    num_of_masks = len(everything_results[0])\n",
    "    print(num_of_masks)\n",
    "\n",
    "    # Display images with matplotlib\n",
    "    fig, axes = plt.subplots(nrows=int(np.ceil(num_of_masks / 6)), ncols=6, figsize=(10, 5))\n",
    "\n",
    "    # Flatten the axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    final_dict = {\n",
    "        \"Red\": 0,\n",
    "        \"Orange\": 0,\n",
    "        \"Yellow\": 0,\n",
    "        \"Green\": 0,\n",
    "        \"Blue\": 0,\n",
    "        \"Brown\": 0,\n",
    "        \"White\": 0,\n",
    "    }\n",
    "    for index, r in enumerate(everything_results[0]):\n",
    "        maskCoords = (r.masks.xy)[0]\n",
    "        xy_array = np.array(maskCoords)\n",
    "\n",
    "        CIRCULAR_THRESHOLD = 0.75\n",
    "        \n",
    "        # Checks if the mask is circular enough\n",
    "        if(check_circularity(xy_array) > CIRCULAR_THRESHOLD):   \n",
    "            contour = xy_array.reshape((-1, 1, 2)).astype(np.int32)\n",
    "            # Create binary mask from contour\n",
    "            mask = np.zeros(raw_image.shape[:2], dtype=np.uint8)\n",
    "            cv2.fillPoly(mask, [contour], 255)\n",
    "\n",
    "            # Apply mask to image\n",
    "            masked_image = cv2.bitwise_and(raw_image, raw_image, mask=mask)\n",
    "\n",
    "            # Get bounding box just to determine region of interest\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            result_image = masked_image[y:y+h, x:x+w]\n",
    "\n",
    "            # Get average RGB and classify it as a color\n",
    "            avg_rgb = get_average_color(result_image)\n",
    "            color_category = classify_color(avg_rgb)\n",
    "            final_dict[color_category] += 1\n",
    "\n",
    "            ax = axes[index]\n",
    "            ax.axis(\"off\")\n",
    "            ax.imshow(result_image)\n",
    "\n",
    "    print(final_dict)\n",
    "\n",
    "    # Display results in the GUI output box\n",
    "    output_text.delete(\"1.0\", tk.END)  # Clear previous text\n",
    "    output_text.insert(tk.END, final_dict)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle uploading the image\n",
    "def uploadImage():\n",
    "    global file_path\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        filetypes=[(\"Image Files\", \"*.png;*.jpg;*.jpeg;*.bmp;*.gif\")]\n",
    "    )\n",
    "    if file_path:\n",
    "        img = Image.open(file_path)\n",
    "        img.thumbnail((300, 300))  # Resize the image to fit in the window\n",
    "        img_tk = ImageTk.PhotoImage(img)\n",
    "        image_label.config(image=img_tk)\n",
    "        image_label.image = img_tk\n",
    "        file_path_label.config(text=f\"File: {file_path}\")\n",
    "\n",
    "        # Display some information in the text box\n",
    "        output_text.delete(\"1.0\", tk.END)  # Clear previous text\n",
    "        output_text.insert(tk.END, f\"File Path: {file_path}\\n\")\n",
    "        output_text.insert(tk.END, f\"Image Size: {img.size}\\n\")\n",
    "        output_text.insert(tk.END, f\"Image Format: {img.format}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script for GUI\n",
    "\n",
    "# Initialize the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Image and Info Display GUI\")\n",
    "root.geometry(\"400x600\")\n",
    "\n",
    "file_path = None\n",
    "\n",
    "# Upload image button\n",
    "upload_button = ttk.Button(\n",
    "    root, text=\"Upload Image\", command=uploadImage\n",
    ")\n",
    "upload_button.pack(pady=10)\n",
    "\n",
    "# Upload process button\n",
    "upload_button = ttk.Button(\n",
    "    root, text=\"Process Image\", command=lambda: processImage(file_path, output_text)\n",
    ")\n",
    "upload_button.pack(pady=10)\n",
    "\n",
    "# Image display label\n",
    "image_label = tk.Label(root)\n",
    "image_label.pack(pady=10)\n",
    "\n",
    "# File path label\n",
    "file_path_label = tk.Label(root, text=\"No file selected\", wraplength=300)\n",
    "file_path_label.pack()\n",
    "\n",
    "# Text box for output information\n",
    "output_text = tk.Text(root, height=10, width=40, state=tk.NORMAL)\n",
    "output_text.pack(pady=10)\n",
    "\n",
    "# Run the main loop\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"bib\">Bibliography</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]\tZ. Peng, J. Li, H. Hao, and Y. Zhong, “Smart structural health monitoring using computer vision and edge computing,” Engineering Structures, vol. 319, p. 118809, Nov. 2024, doi: [10.1016/j.engstruct.2024.118809](https://doi.org/10.1016/j.engstruct.2024.118809).\n",
    "\n",
    "[2]\tUltralytics, “FastSAM (Fast Segment Anything Model).” Accessed: Dec. 19, 2024. [Online]. Available: https://docs.ultralytics.com/models/fast-sam\n",
    "\n",
    "[3]\tCASIA-IVA-Lab/FastSAM. (Dec. 19, 2024). Python. CASIA-IVA-Lab. Accessed: Dec. 19, 2024. [Online]. Available: https://github.com/CASIA-IVA-Lab/FastSAM\n",
    "\n",
    "[4]\tUltralytics, “SAM (Segment Anything Model).” Accessed: Dec. 19, 2024. [Online]. Available: https://docs.ultralytics.com/models/sam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
